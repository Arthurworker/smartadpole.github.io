---
layout: article
title:  "「DL」 GAN 预备知识"
date:   2019-04-09 15:08:40 +0800
key: gan-base-dl-20190409
aside:
  toc: true
category: [DL, GAN]
sidebar:
  nav: GAN
---

>文章将简略介绍 GAN 中用到的深度学习概念；    


<!--more-->
参考[^1]·[^2]·[^3]·[^4]·[^5]·[^6]·[^7]   

# 1 技能
- **编程语言**：python，python 第三方库（numpy，pandas，matplotlib/seaborn）；  
- **工具**：pip，jupyter，docker（可选）   
- **深度学习框架**：Tensorflow，MXNet，Pytorch，Keras，Caffe，Caffe2 中的一种；   

# 2 术语
## 2.1 卷积网络
- 全连接  
多层感知机
- 卷积  
下采样，权重共享，步长，跨步卷积，特征图    
- 反卷积/转置卷积   
上采样；  
在超分辨率，分割，深度估计，光流计算，自编码，GAN等场景中有这广泛应用；*（事实上需要输出图像的地方都需要他）*   
- 池化   
- RNN & LSTM   
- 损失函数   

## 2.2 数学
1. **纳什均衡**    
一个经济学和博弈论的术语，代表一着个系统的稳定状态，此时系统的任何一个参与者都无法通过各自行动的改变而增加收益；   
<center class="half">
  <iframe width="100%" height="380" src="/assets/images/ml/dl/GAN/barrel-effect.gif">木桶效应</iframe>&emsp;
</center>   

1. **概率分布**   

1. **噪声**   

1. **高斯分布**   
正态分布   
\begin{equation}
\mathcal N(x; \mu, \sigma^2) = \sqrt{\frac{1}{2\pi \sigma^2}} \exp \left ( -\frac{1}{2\sigma^2} (x-\mu)^2 \right )  
\end{equation}
常在自然和社会科学中代表一个不明的随机变量[^8]:   
- **真实情况下许多模型的分布都接近正态分布**；中心极限定理说明了很多独立随机变量的和接近正态分布；   
- **具有相同方差的所有可能的概率分布中，正态分布在实数上具有最大的不确定性**；也就是说正态分布是对模型加入先验知识最少的分布；       

1. **期望**   
函数 $f(x)$ 关于概率分布 $P(\mathtt x)$ 的期望（expectation）（*即变量 $x$ 由概率分布 $P(x)$ 产生时，$f$ 作用与 $x$ 时，$f(x)$ 的平均值*）:   
\begin{equation}\label{EXP_discrete}   
{\mathbb E}_{\mathtt x \sim P}[f(x)] = \sum_x {P(x)f(x)} \quad     
\end{equation}   

\begin{equation}\label{EXP_variables}   
{\mathbb E}_{\mathtt x \sim p}[f(x)] = \int {P(x)f(x)dx} \quad     
\end{equation}    

  $\eqref{EXP_discrete}$ 是离散型，$\eqref{EXP_variables}$ 是连续型；   
  *条件明确时，$\mathbb {E}_{\mathtt x \sim P}[f(x)]$ 会简化成 $\mathbb {E}_{\mathtt x}[f(x)]$ 、$\mathbb {E}[f(x)]$ 或 $\mathbb {E}$*   


1. ****   

1. ****   

-------------------  
[End](#1-技能)
{:.warning}  


# 附录
## A 参考资料
[^1]: ciic. KL散度、JS散度、Wasserstein距离[EB/OL]. <https://zxth93.github.io/2017/09/27/KL%E6%95%A3%E5%BA%A6JS%E6%95%A3%E5%BA%A6Wasserstein%E8%B7%9D%E7%A6%BB/index.html>. 2017-09-27/2019-04-09.    
[^2]: denny402. 概率分布之间的距离度量以及python实现(四)[EB/OL]. <https://www.cnblogs.com/denny402/p/7054950.html>. 2017-06-20/2019-04-09.    
[^3]: 夕月一弯. 概率分布之间的距离度量以及python实现[EB/OL]. <https://www.cnblogs.com/wt869054461/p/7156397.html>. 2017-07-12/2019-04-09.     
[^4]: 大奸猫. EMD距离wasserstein_distance的使用[EB/OL]. <https://blog.csdn.net/yeziand01/article/details/84404383>. 2018-11-23/2019-04-09.     
[^5]: 黄若孜. Wasserstein距离在生成模型中的应用[EB/OL]. <https://zhuanlan.zhihu.com/p/35879231>. 2018-04-19/2019-04-09.     
[^6]: -. wasserstein 距离的问题[EB/OL]. <https://www.zhihu.com/question/41752299>. -/2019-04-09.    
[^7]: Herbert_Zero. 一种度量准则：EMD[EB/OL]. <https://blog.csdn.net/liyuefeilong/article/details/45891945>. 2015-05-21/2019-04-09.    
[^8]: Ian Goodfellow, et. al. 著 赵申剑 等译. 深度学习[M]. 北京:人民邮电出版社, 2017.
