---
layout: article
title:  "「DL」 GAN 预备知识"
date:   2019-04-09 15:08:40 +0800
key: gan-base-dl-20190409
aside:
  toc: true
category: [DL, GAN]
sidebar:
  nav: GAN
---

>文章将简略介绍 GAN 中用到的深度学习概念；    


<!--more-->
参考[^1]·[^2]·[^3]·[^4]·[^5]·[^6]·[^7]   

# 1 技能
- **编程语言**：python，python 第三方库（numpy，pandas，matplotlib/seaborn）；  
- **工具**：pip，jupyter，docker（可选）   
- **深度学习框架**：Tensorflow，MXNet，Pytorch，Keras，Caffe，Caffe2 中的一种；   

# 2 术语
## 2.1 卷积网络
- 全连接  
多层感知机
- 卷积  
下采样，权重共享，步长，跨步卷积，特征图    
- 反卷积/转置卷积   
上采样；  
在超分辨率，分割，深度估计，光流计算，自编码，GAN等场景中有这广泛应用；*（事实上需要输出图像的地方都需要他）*   
- 池化   
- RNN & LSTM   
- 损失函数   

## 2.2 数学
### 2.2.1 初等数学
1. **对数**    
和差公式：$\log M \cdot N = \log M + \log N$，$\log \frac{M}{N} = \log M - \log N$  
次方公式：$\log_{\alpha^n} x^m = \frac{m}{n} \log_\alpha x$  

### 2.2.2 微积分
1. **运算法则**    
$\int [f(x) \pm g(x)] dx = \int f(x) dx \pm \int g(x) dx$   

### 2.2.3 概率论
1. **随机变量**   
给定样本空间 $S$，如果其上的实值函数 $X:S$ 是实值可测函数，则称 $X$ 为随机变量；称其为变量是指可作为因变量；     
`谁可以作为因变量，函数值还是函数`{:.warning}    

1. **概率分布**   
概率累加所得，故又叫「累积分布函数」；   
设 $P$ 为概率， $X$ 为随机变量，则函数 $F(x) = P(X \leq x) \quad   (x\in \mathbb R)$ 称为 $X$ 的概率分布函数；如果将 $X$ 看成是数轴上的随机点的坐标，那么，分布函数 $F(x)$ 就表示 $X$ 落在区间 $(-\infty, x]$ 上的概率；   
*$F(x) = P(X \leq x)$ 也会写作 $F_X(x)$*    

1. **概率密度函数**   
为连续随机变量定义的，本身不是概率，在 $x$ 上积分后才是概率：      
对于一维随机变量 $X$，设它的概率分布是 $F_X(x)$，满足：   
$ \forall -\infty <a<\infty ,\quad F_X(a) = \int _{-\infty }^{a} f_X(x)\,dx$    
那么 $f_X(x)$ 是它的概率密度函数；

1. **概率质量函数**   
为离散随机变量定义的，等于概率：$p(x) = P(X=x)$      

1. **期望**   
函数 $f(x)$ 关于概率分布 $P(\mathtt x)$ 的期望（expectation）（*即变量 $x$ 由概率分布 $P(x)$ 产生时，$f$ 作用与 $x$ 时，$f(x)$ 的平均值*）:   
$$
\begin{align}   
{\mathbb E}_{\mathtt x \sim P}[f(x)] &= \sum_x {P(x)f(x)} \quad   \label{EXP_discrete}\\     
{\mathbb E}_{\mathtt x \sim p}[f(x)] &= \int {P(x)f(x)dx} \label{EXP_variables}     
\end{align}    
$$   
$\eqref{EXP_discrete}$ 是离散型，$\eqref{EXP_variables}$ 是连续型；   
*条件明确时，$\mathbb {E}_{\mathtt x \sim P}[f(x)]$ 会简化成 $\mathbb {E}_{\mathtt x}[f(x)]$ 、$\mathbb {E}[f(x)]$ 或 $\mathbb {E}$*   

1. **高斯分布**   
正态分布   
\begin{equation}
\mathcal N(x; \mu, \sigma^2) = \sqrt{\frac{1}{2\pi \sigma^2}} \exp \left ( -\frac{1}{2\sigma^2} (x-\mu)^2 \right )  
\end{equation}
常在自然和社会科学中代表一个不明的随机变量[^8]:   
- **真实情况下许多模型的分布都接近正态分布**；中心极限定理说明了很多独立随机变量的和接近正态分布；   
- **具有相同方差的所有可能的概率分布中，正态分布在实数上具有最大的不确定性**；也就是说正态分布是对模型加入先验知识最少的分布；       
*$N(x; \mu, \sigma^2)$ 在有些文献中也写作 $N(x\|\mu, \sigma^2)$*    

1. **最大似然估计**   
利用已知的样本，反推最有可能（最大概率）导致这样结果的参数值（概率分布）；    
对于样本 $D=\{x_1, x_2, \cdots, x_n\}$ 的 $\theta$ 的似然函数为：   
$$
\begin{align}   
l(\theta) &= p(D;\theta) \\
&= p(x_1, x_2, \cdots, x_n;\theta) \\
&= \prod_{i=1}^N p(x_i;\theta) \label{Likelihood}  
\end{align}
$$
`为什么 似然计算时要用乘而不是加`{:.warning}   
**最大似然估计**：那么就存在一个 $\theta$ 使似然函数 $l(\theta)$ 的值最大：   
\begin{equation}
\theta^* = \arg \max_\theta l(\theta) = \arg \max_\theta \prod_{i=1}^N p(x_i;\theta)
\end{equation}

1. **KL 散度**  
相对熵，是两个概率分布 $P$ 和 *Q* 差异的度量，是非对称的；*P* 表示数据的真实分布，*Q* 表示模型分布或 *P* 的近似分布；
$$
\begin{align}
D_{\mathrm  {KL}}(P\|Q) &= \sum_{i}P(i)\ln {\frac  {P(i)}{Q(i)}} \label{kl_discrete} \\
D_{\mathrm  {KL}}(P\|Q) &= \int_{-\infty }^{\infty} p(x)\ln {\frac  {p(x)}{q(x)}} dx \label{kl_divergence}
\end{align}
$$   
$\eqref{kl_discrete}$ 是离散型，$\eqref{kl_divergence}$ 是连续型；   

1. **JS 散度**  
也是度量两个概率分布的相似度，具有对称性，取值在 0 到 1 之间；   
$$
\begin{align}
D_{\mathrm  {JS}}(P\|Q) = \frac{1}{2} KL(P\| \frac{P+Q}{2} + \frac{1}{2} KL(Q\| \frac{P+Q}{2})
\end{align}   
$$   
两个分布 $P$, $Q$ 离得很远，完全没有重叠的时候，那么 KL 散度值是没有意义的，而 JS 散度值是一个常数；   

1. **Wasserstein 距离**  
度量两个概率分布的距离；    
$$
\begin{align}
W_{n}(P ,Q) &= \left(\inf_{\gamma \in \Gamma(P, Q)} \int_{M \times M} d(x,y)^n d\gamma (x,y) \right)^{1/n} \\
W(P ,Q) &= \inf_{\gamma \in \Gamma(P, Q)} \mathtt E_{x, y} \sim \gamma[\lVert{x-y}\rVert]
\end{align}   
$$     
$\Gamma(P, Q)$ 是分布 $P$ 和 $Q$ 所有可能的联合分布；从该分布中采样到样本对 $(x, y)$，取到样本距离的最小均值，就是 Wasserstein 距离；直观上来看就是分布 $P$ 变换到分布 $Q$ 时在最优路径规划下的最小消耗；所以 Wesserstein 距离又叫 Earth-Mover 距离或推土机距离；  
Wessertein 距离的优势在于：即使两个分布没有重叠或者重叠非常少，仍然能反映两个分布的远近；   

### 2.2.4 博弈论
1. **纳什均衡**    
一个经济学和博弈论的术语，代表一着个系统的稳定状态，此时系统的任何一个参与者都无法通过各自行动的改变而增加收益；   
<center class="half">
  <iframe width="100%" height="380" src="/assets/images/ml/dl/GAN/barrel-effect.gif">木桶效应</iframe>&emsp;
</center>   


-------------------  
[End](#1-技能)
{:.warning}  


# 附录
## A 参考资料
[^1]: ciic. KL散度、JS散度、Wasserstein距离[EB/OL]. <https://zxth93.github.io/2017/09/27/KL%E6%95%A3%E5%BA%A6JS%E6%95%A3%E5%BA%A6Wasserstein%E8%B7%9D%E7%A6%BB/index.html>. 2017-09-27/2019-04-09.    
[^2]: denny402. 概率分布之间的距离度量以及python实现(四)[EB/OL]. <https://www.cnblogs.com/denny402/p/7054950.html>. 2017-06-20/2019-04-09.    
[^3]: 夕月一弯. 概率分布之间的距离度量以及python实现[EB/OL]. <https://www.cnblogs.com/wt869054461/p/7156397.html>. 2017-07-12/2019-04-09.     
[^4]: 大奸猫. EMD距离wasserstein_distance的使用[EB/OL]. <https://blog.csdn.net/yeziand01/article/details/84404383>. 2018-11-23/2019-04-09.     
[^5]: 黄若孜. Wasserstein距离在生成模型中的应用[EB/OL]. <https://zhuanlan.zhihu.com/p/35879231>. 2018-04-19/2019-04-09.     
[^6]: -. wasserstein 距离的问题[EB/OL]. <https://www.zhihu.com/question/41752299>. -/2019-04-09.    
[^7]: Herbert_Zero. 一种度量准则：EMD[EB/OL]. <https://blog.csdn.net/liyuefeilong/article/details/45891945>. 2015-05-21/2019-04-09.    
[^8]: Ian Goodfellow, et. al. 著 赵申剑 等译. 深度学习[M]. 北京:人民邮电出版社, 2017.
