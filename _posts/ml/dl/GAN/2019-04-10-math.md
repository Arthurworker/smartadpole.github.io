---
layout: article
title:  "「DL」 GAN 的数学原理"
date:   2019-04-10 14:08:40 +0800
key: math-in-gan-20190410
aside:
  toc: true
category: [DL, GAN]
sidebar:
  nav: GAN
---

>


<!--more-->

# 最大似然估计
最大似然估计与 KL 散度关系的推导；   
两个分布 $P, Q$，$x = { x_1, x_2, \cdots, x_m}$ 为分布 $Q$ 下的样本；   
公式 $\eqref{Likelihood}$ 为似然函数，公式 $\eqref{kl_divergence}$ 为 KL 散度；   
$$
\begin{align}   
l(\theta) &= \prod_{i=1}^m P(x_i;\theta) \label{Likelihood}\\
D_{\mathrm  {KL}}(P\|Q) &= \int P(x)\ln {\frac{P(x)}{Q(x)}} dx \label{kl_divergence}
\end{align}
$$

$$
\begin{align}   
\theta^* &= \arg \max_\theta l(\theta) \\
&= \arg \max_\theta \prod_{i=1}^m P(x_i;\theta) \\
&= \arg \max_\theta \log \prod_{i=1}^m P(x_i;\theta) \\
&= \arg \max_\theta \sum_{i=1}^m \log P(x_i;\theta) \\
&= \arg \max_\theta \frac{1}{m} \sum_{i=1}^m \log P(x_i;\theta) \\
&= \arg \max_\theta \mathbb E_{x \sim Q} \log P(x;\theta) \\
&= \arg \max_\theta \int Q(x) \log P(x;\theta) dx \\
&= \arg \max_\theta \int Q(x) \log P(x;\theta) dx - \int Q(x) \log P(x) dx\\
&= \arg \max_\theta \left(\int Q(x) \log P(x;\theta) - \int Q(x) \log P(x)) dx \right)\\
&= \arg \max_\theta \int Q(x) (\log P(x;\theta) - \log P(x)) dx\\
&= \arg \max_\theta \int Q(x) (\log \frac{P(x;\theta)}{P(x)} dx\\
&= D_{\mathrm  {KL}}(Q\|P)
\end{align}
$$

# 损失函数
GAN 的 loss 函数等于 JS 散度的推到；  

既然 GAN 的 loss 是 JS 散度，那为什么不直接用 JS 来做；   

-------------------  
 End
{:.warning}  


# 附录
## A 参考资料
