---
layout: article
title:  "「DL」 梯度下降"
date:   2020-07-11 00:08:40 +0800
key: optimization
aside:
  toc: true
category: [AI, DL, foundation]
---
<span id='head'></span>  
>[优化器资源](/ai/dl/foundation/2019/05/20/foundation.html#41-优化器)     

<!--more-->

# 1 概念
梯度方向导数最大；    
经验风险最小化：最小化平均误差；     
批量梯度下降：一次迭代使用所有训练数据；   
小批量梯度下降：一次迭代使用部分训练数据；   
随机批量梯度下降：一次迭代使用一个训练数据；现在常用来表示小批量梯度下降；        


# 2 挑战

| 挑战 | 描述 | 监测 | 方案 |
| --- | --- | --- | --- |
| 病态 | 数值优化中普遍的问题，即使很小的更新步长也会增加代价函数； | 梯度的2范数在持续增加，:ghost: | 减小学习率缓解<br>牛顿法（不适合CNN） |    
| 局部极小值 |  | 梯度的范数（可能） | 更多的数据；<br>更小的网络；<br>没那么重要 |
| 平坦区域（高原、鞍点） | CNN 出现鞍点的概率是局部极小点的指数倍； |  | 一阶对鞍点适应性较好； |
| 悬崖、梯度爆炸 | 循环网络中最常见 |  | 梯度截断 |
| 长期依赖 | 同一数据被反复累乘 |  | 易造成梯度消失和爆炸 |
|  |  |  |  |

*自编码网络只有全局极小和鞍点，没有局部极小；因为输入到输出是线性映射；*`为啥线性就不会有局部极小值；也不是严格的线性吧`{:.warning}    
*时间步长上的累乘，造成循环网络易出现悬崖式梯度*    

# 3 一阶
>梯度下降   

## 3.1 基础算法
SGD 能轻易逃脱鞍点`为什么`{:.warning}；   
动量：主要解决病态问题和随机梯度的方差；可以很好的穿过狭长的峡谷；而 SGD 容易在窄轴上来回震荡；   
Nesterov动量：使用更新后的权重计算梯度；在批量梯度中收敛速度减少，但 SGD 中没有变化；    


## 3.2 自适应学习率

| 方法 | 特色 | 策略 |
| --- | --- | --- |
| AdaGrad |  | 学习率自适应反比于累计梯度（计算时用了平方和的平方根）|
| RMSProp | 更好应对非凸 | 使用指数衰减平均代替简单的累计，以防止到达凸结构之前就将学习率衰减太小，导致学习缓慢； |
| Adam | 对超参更鲁棒 | 将修正操作同时作用于一阶矩和二阶矩； |

*关于优化算法的选择尚没有确定的结论；*     

# 4 二阶近似
牛顿法；缺点是无法适应鞍点问题，也有一些方案，如无鞍点牛顿法；但二阶在 CNN 中应用仍少；当然，对于极大值也无法适应；       
共轭梯度；   
BFGS；   

-------------------  
[End](#head)
{:.warning}  

# 附录
## A 参考资料
