---
layout: article
title:  "「CV」 目标检测概述"
date:   2020-06-04 16:00:40 +0800
key: ObjectDetection-survey
aside:
  toc: true
category: [AI, CV, detection]
---
<span id='head'> </span>
[目标检测资源](/ai/cv/detection/2019/05/10/foundation.html)     

<center class="half">
  <img src="/assets/images/cv/detection/overview.png" /><br>图1：目标检测发展路线&emsp;
</center>

<!--more-->  

# 1 技术点

# 2 经典网络
## 2.1 基于 anchor
### 2.1.1 two stage  

<center class="half">
  <img src="/assets/images/cv/detection/two_stage.png" /><br>图2：两阶段算法演进路线&emsp;
</center>


| 模型 | 意义 | 创新点 |
| --- | --- | --- |
| RCNN | 开创者 | 首次证明在目标检测领域深度学习方法的有效性； <br>检测精度比传统特征（Hog特征）精度更高 |
| Fast RCNN | 速度 | 使用 RoIPooling 实现区域间特征共享；<br>使用多任务损失（分类和框回归）实现了端到端的训练；|
| Faster RCNN | 速度精度 | 用 RPN 代替选择性搜索，速度提升，且与检测网络共享特征图；<br>用多尺度 anchor 来应对多尺度目标检测；无需输入多尺度特整体即可完成多尺度目标检测；|
| Mask RCNN | 精度 | 加入分割任务，同时输出检测和分割结果；<br>特征提取用 ResNet -FPN (feature pyramid network)，并在每个金字塔层独立作出预测，提升了精度；<br>RoIAlign 替换 RoIPooling；  |

### 2.1.2 one stage

| 模型 | 意义 | 创新点 |
| --- | --- | --- |
| YOLO | 实时检测 | 将输入划分为 S×S 网格，网格负责中心点落在网格内的目标；<br>框预测和分类都用回归来做； |
| YOLOv2 | 训练加速和精度 | 加了 BN<br>高分辨率输入：448×448<br>引入了 anchor，去掉了预测框用的全连接层<br>从训练集聚类得到 anchor 参数<br>多尺度特征融合<br>多尺度训练<br>Darknet 19 |
| YOLOv3 | 小目标 | 多标签分类（独立的分类器）适应重迭标签<br>在三种尺度特征图上做预测<br>特征提取用 Darknet-53 |
| Retinanet | 样本不均衡 | 提出了 focal loss<br>ResNeXt-101-FPN |
| RefineDet | 小目标 | 检测分两步走 |
| M2Det | 多尺度目标 | MLFPN |

`为什么会选用不同的指标`{:.warning}     
`YOLO 和 SSD 两个系列的区别是什么`{:.warning}    


| 模型     | 创新点 | backbone | neck | head | 缺点 |
| ---     | --- | --- | --- | --- | --- |
| SSD     | 正负样本不均衡<br>速度和精度的平衡 | VGG16 | 原始特征图 | 多层预测 | 小物体效果差<br>anchor要要手动设定<br>精度比两阶段低|
| DSSD    | 小目标检测 | SSD+ResNet | 反卷积+高-低相乘| 残差 | 巨慢 |
| RSSD    | 小目标检测<br>误检| SSD+ResNet | 上下同时融合 + concate | 分类参数共享 | 慢 |
|  | | | | | |
| | | | | | |

## 2.2 anchor free

| 模型 | 意义 | 创新点 |
| --- | --- | --- |
| RelationNet | 物体间的关系 | 提出目标关系模块 |
| DCNv2 | 几何形变问题 | 提出可变形卷积网络 DCN |
| NAS-FPN | 精度 | 自动搜索出的 FPN 结构 |

# 3 难点
## 3.1 常规组件
### 3.1.1 骨干网络

### 3.1.2 后处理

## 3.2 精度
### 3.2.1 样本不均衡

### 3.2.2 过拟合

### 3.2.3 多尺度

### 3.2.4 遮挡&拥挤


## 3.3 性能
### 3.3.1 [轻量化模型](/ai/dl/cnn/2020/06/27/light-cnn.html)
### 3.3.2 模型压缩


# 4 优化方向

| 方向 | 策略 |
| --- | --- |
| backbone |  |
| 优化策略 | UnitBox，IoUNet |
| 损失函数 | L1,L2,focal loss |
| NMS | soft-nms，softer-nms，relation net，ConvNMS，NMSNet，YesNet |
| anchor 生成 | 滑窗，RPN，CornerNet，meta-anchor |
| 零样本 |  |
|  |  |
|  |  |
|  |  |


# 5 发展
小目标的检测(如小于30像素的目标物体)、遮挡面积较大的目标以及区分图像中与目标物体外形相似的非目标物体等问题;   
实时性:对于自动驾驶或汽车辅助驾驶等场景对实时处理能力要求较高；   
小数据量：微调的精度尚待提高；    
特定行业很难获取大量的监督数据；    

-------------------  
[End](#head)
{:.warning}  


# 附录
## A 经典网络
### a 两阶段

### b 单阶段
<span id='SSD'>  </span>

**SSD**
{:.warning}
```
backbone： VGG16(conv4-3 之后的去掉了)   
neck： 单层特征图     
head： 多层预测；添加了c8（10×10）、c9（5×5）、c10（3×3）、c11（1×1） 联合 conv4-3（19×19） 一起做预测；    
添加的几层是先用 1×1 降通道，然后再降分辨率；     
损失函数： softmax-交叉熵，SmoothL1   
prior box：    
```
**triker：**    
1. 解决正负样本不均衡    
通过筛选控制正负比例在 1：3；    
前向传播完成后，将预测框和真值框按一定策略匹配（参见 issue 1），这样得到了正负样本；之后选所有正样本（通常<100），并按置信度对负样本降序排序，筛选出靠前的3倍于正样本数量的负样本；然后再对筛选所得的框进行反向传播；*反向传播时只取正样本的偏移量，负样本的舍弃；*    
1. 丰富的数据增强；   
颜色抖动，crop；     
1. 所有类别共享一个回归值，FasterRCNN 是不同类别用不同回归值；    
FasterRCNN 是从 FastRCNN 继承得来的分类策略，最终的 box 回归时，为每一个类别都回归了一个 box；而此处是直接 n 分类 + 1 个 box 回归值；       

**缺点：**    
1. 小物体检测不好，低层特征虽然有利于定位，但是语义信息不够，所以分类不够；即没有考虑特征图之间的关联；   
1. 先验框尺寸需要手动设定，要根据不同任务去修改`怎么改，聚类吗`{:.warning}     
1. 一阶段算法的精度还是比两阶段的低；   
1. 多个特征图在检测同一物体，NMS 后仍然不能去除误检；`难道同一层的 Anchor 就不会检测到同一物体了，归罪于多层独立是不是不合理`{:.warning}     

**issue：**     
1. default box 与 真是框的匹配策略     
预测框绑定到与其 IoU 最大的真实框上，真实框要与其 IoU >0.5 的预测框绑定；未发生绑定的即为负样本；             
1. 为什么说借鉴了 FasterRCNN 的 RPN 思路；    
anchor 机制：RPN 中特征图上一个点对应原图中的一组框，只不过这组框在 SSD 中更名为 prior box；                          
1. 一阶段和两阶段的区别？   
两阶段：先进行 RPN（对 box 粗回归 + 前背景分类），筛选过后，用 ROIPooling 将目标对应的特征图裁剪出来，裁剪结果依次进行二次回归 + 多分类；    
单阶段一步到位，直接进行多分类 + box 回归；  
1. 框是怎么映射回去的    
不需要映射，anchor 本身就有宽高，回归的结果是比例，直接修正就可以；也就是说，预测值本身就是针对原始图片做的；    
1. 为什么不用 IoU 做损失    
可以用，而且抗尺度影响；    

*派生算法有：DSSD（上采样特征融合）、RSSD（彩虹特征融合）、RefineDet（FasterRCNN 结合 SSD）、RFBNet（多个感受野融合）*     

<span id='DSSD'>  </span>

**DSSD**
{:.warning}
使用特征金字塔缓解小目标检测问题；    
```
backbone： SSD + ResNet101     
neck： 特征融合（反卷积+相乘）     
head： 使用了残差结构    
```  
结果：效率降低了 7 倍，精度就提升了 1 个点不到；  

1. 深层反卷积，浅层叠了两层标配卷积，然后再相乘的；*加了好多参数量,加法融合后 mAP 提高了 1.3，乘法再提高 0.2*     
1. 预测时的残差结构令 mAP 提升了 0.5 ～ 0.7；   

*特征融合包括：逐元素相加（FPN）、逐元素相乘（DSSD）、通道拼接*    
>
这么多的参数量，才带来 2 个百分点的提升，工业中很不值得     
对于 VOC 数据集只提升了 2 个点，但不知道实际应用中和 SSD 的差距会有多少；    

<span id='DSSD'>  </span>

**RSSD**
{:.warning}
使用特征金字塔缓解小目标检测问题；并使用彩虹连接缓解误检问题；    
```
backbone： SSD + ResNet101     
neck： 彩虹融合；     
head： 用同一个分类网络（所有框共用一套分类参数），更稳定，收敛更快；
```    

**结果**：300 的输入时，效率比 SSD 降了一半，精度也没见提高啊；512 的输入倒是提高了 1 个点；单独的小目标召回确实高了2～4个点；  

**彩虹融合**：通常下采样用来将低层特征逐层堆叠到高层去，上采样则是将高层特征逐层堆叠到低层；此处就用 pooling 和反卷积同时将上下层特征图拼接到当前层，那么所有特征图的通道数就一样了；   
特征融合前用了 BN，达到归一化感受野和不同尺度的效果`怎么就达到这个效果了，确定不是为了强行使用 BN 而杜撰的概念`{:.warning}

**issue**   
1. 最后怎么做的分类，确定不是参数多了之后引起的分类更准确？     
其实 SSD 中最后分类时，需要针对每一张特征图定义一个分类层，反向传播时自然也是单独训练；因为这些分类层不改变特征图尺寸，只改变通道数，姑且认为等于分类数量，也就意味着最终经过分类层/预测层处理后的特征图通道数是一样的；而这里，因为彩虹融合使得每个用于预测的特征图通道相同，那么在输入输出通道数一样的情况下，及可以将多个分类层可以合并成一个；
>且不说效果怎么样，至少参数量是又降低了的；    

1. 和 DSSD 相比，同样使用了 ResNet101，为啥这个速度快了很多    
难道只是因为用 concate 代替了乘法，速度就提升了 4 倍？    

### c free anchor

## B 参考资料
1. [目标检测比赛中的tricks](https://zhuanlan.zhihu.com/p/102817180)
1. [目标检测算法中检测框合并策略技术综述](https://zhuanlan.zhihu.com/p/48169867)     
1. [目标检测比赛提高mAP的方法](https://www.cnblogs.com/zi-wang/p/12537034.html)    
1. [目标检测入门，看这篇就够了](https://zhuanlan.zhihu.com/p/34142321)-glint     
