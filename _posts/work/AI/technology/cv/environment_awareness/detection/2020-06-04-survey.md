---
layout: article
title:  "「CV」 目标检测概述"
date:   2020-06-04 16:00:40 +0800
key: ObjectDetection-survey
aside:
  toc: true
category: [AI, CV, detection]
---
<span id='head'> </span>
[目标检测资源](/ai/cv/detection/2019/05/10/foundation.html)     

<center class="half">
  <img src="/assets/images/cv/detection/overview.png" /><br>图1：目标检测发展路线&emsp;
</center>

<!--more-->  

# 1 技术点

# 2 经典网络
## 2.1 基于 anchor
### 2.1.1 two stage  

<center class="half">
  <img src="/assets/images/cv/detection/two_stage.png" /><br>图2：两阶段算法演进路线&emsp;
</center>


| 模型 | 意义 | 创新点 |
| --- | --- | --- |
| RCNN | 开创者 | 首次证明在目标检测领域深度学习方法的有效性；|
| Fast RCNN | 速度 | RoIPooling；<br>多任务损失端到端的训练；|
| Faster RCNN | 速度精度 | RPN|
| Mask RCNN | 精度 | 加入分割<br>ResNet + FPN + SSD<br>RoIAlign  |

### 2.1.2 one stage



`为什么会选用不同的指标`{:.warning}     
`YOLO 和 SSD 两个系列的区别是什么`{:.warning}    


| 模型     | 目标 | backbone | neck | head | 缺点 |
| ---     | --- | --- | --- | --- | --- |
| [SSD](#SSD)   | 正负样本不均衡<br>速度和精度的平衡 | VGG16 | 原始特征图 | 多层 | 小物体效果差<br>anchor要要手动设定<br>精度比两阶段低|
| [DSSD](#DSSD) | 小目标检测 | ResNet | 反卷积+高-低相乘| 多层 + 残差 | 巨慢 |
| [RSSD](#RSSD) | 小目标检测<br>误检| ResNet | 上下同时融合 + concate | 多层 <br> 分类共享 | 慢 |
| [RefineDet](#RefineDet) | 精度 | ResNet | FPN | 多层<br>RPN |  |
| [YOLOv1](#YOLOv1) | 速度 | darknet24 | - | 单层 | 分类、定位精度低<br>拥挤情况效果差 |
| [YOLOv2](#YOLOv2) | 精度 | darknet19 + anchor | pooling + concate | 单层 | 同上 |
| [YOLOv3](#YOLOv3) | 精度 | darknet53 + anchor | uppooling + concate | 多层<br>sigmoid | 同上、慢 |
| [YOLOv4](#YOLOv4) | 精度 | - | - | - | - |
| [RetinaNet](#RetinaNet) | 样本不均衡 | ResNet | FPN | 多层预测<br>sigmoid |  |
| M2Det | 多尺度目标 | VGG | MLFPN | 多层 | 慢 |
|  |  |  |  |  |  |

## 2.2 anchor free

| 模型 | 意义 | 创新点 |
| --- | --- | --- |
| RelationNet | 物体间的关系 | 提出目标关系模块 |
| DCNv2 | 几何形变问题 | 提出可变形卷积网络 DCN |
| NAS-FPN | 精度 | 自动搜索出的 FPN 结构 |

# 3 难点
## 3.1 常规组件
### 3.1.1 骨干网络

### 3.1.2 后处理

## 3.2 精度
### 3.2.1 样本不均衡

### 3.2.2 过拟合

### 3.2.3 多尺度

### 3.2.4 遮挡&拥挤


## 3.3 性能
### 3.3.1 [轻量化模型](/ai/dl/cnn/2020/06/27/light-cnn.html)
### 3.3.2 模型压缩


# 4 优化方向

| 方向 | 策略 |
| --- | --- |
| backbone |  |
| 优化策略 | UnitBox，IoUNet |
| 损失函数 | L1,L2,focal loss |
| NMS | soft-nms，softer-nms，relation net，ConvNMS，NMSNet，YesNet |
| anchor 生成 | 滑窗，RPN，CornerNet，meta-anchor |
| 零样本 |  |
|  |  |
|  |  |
|  |  |


# 5 发展
小目标的检测(如小于30像素的目标物体)、遮挡面积较大的目标以及区分图像中与目标物体外形相似的非目标物体等问题;   
实时性:对于自动驾驶或汽车辅助驾驶等场景对实时处理能力要求较高；   
小数据量：微调的精度尚待提高；    
特定行业很难获取大量的监督数据；    

-------------------  
[End](#head)
{:.warning}  


# 附录
## A 经典网络
### a 两阶段

### b 单阶段
<span id='SSD'>  </span>

**SSD**
{:.warning}
```
backbone： VGG16(conv4-3 之后的去掉了)   
neck： 单层特征图     
head： 多层预测；添加了c8（10×10）、c9（5×5）、c10（3×3）、c11（1×1） 联合 conv4-3（19×19） 一起做预测；    
添加的几层是先用 1×1 降通道，然后再降分辨率；     
损失函数： softmax-交叉熵，SmoothL1   
prior box：    
```
**triker：**    
1. 解决正负样本不均衡    
通过筛选控制正负比例在 1：3；    
前向传播完成后，将预测框和真值框按一定策略匹配（参见 issue 1），这样得到了正负样本；之后选所有正样本（通常<100），并按置信度对负样本降序排序，筛选出靠前的3倍于正样本数量的负样本；然后再对筛选所得的框进行反向传播；*反向传播时只取正样本的偏移量，负样本的舍弃；*    
1. 丰富的数据增强；   
颜色抖动，crop；     
1. 所有类别共享一个回归值，FasterRCNN 是不同类别用不同回归值；    
FasterRCNN 是从 FastRCNN 继承得来的分类策略，最终的 box 回归时，为每一个类别都回归了一个 box；而此处是直接 n 分类 + 1 个 box 回归值；       

**缺点：**    
1. 小物体检测不好，低层特征虽然有利于定位，但是语义信息不够，所以分类不够；即没有考虑特征图之间的关联；   
1. 先验框尺寸需要手动设定，要根据不同任务去修改`怎么改，聚类吗`{:.warning}     
1. 一阶段算法的精度还是比两阶段的低；   
1. 多个特征图在检测同一物体，NMS 后仍然不能去除误检；`难道同一层的 Anchor 就不会检测到同一物体了，归罪于多层独立是不是不合理`{:.warning}     

**issue：**     
1. default box 与 真是框的匹配策略     
预测框绑定到与其 IoU 最大的真实框上，真实框要与其 IoU >0.5 的预测框绑定；未发生绑定的即为负样本；             
1. 为什么说借鉴了 FasterRCNN 的 RPN 思路；    
anchor 机制：RPN 中特征图上一个点对应原图中的一组框，只不过这组框在 SSD 中更名为 prior box；                          
1. 一阶段和两阶段的区别？   
两阶段：先进行 RPN（对 box 粗回归 + 前背景分类），筛选过后，用 ROIPooling 将目标对应的特征图裁剪出来，裁剪结果依次进行二次回归 + 多分类；    
单阶段一步到位，直接进行多分类 + box 回归；  
1. 框是怎么映射回去的    
不需要映射，anchor 本身就有宽高，回归的结果是比例，直接修正就可以；也就是说，预测值本身就是针对原始图片做的；    
1. 为什么不用 IoU 做损失    
可以用，而且抗尺度影响；    

*派生算法有：DSSD（上采样特征融合）、RSSD（彩虹特征融合）、RefineDet（FasterRCNN 结合 SSD）、RFBNet（多个感受野融合）*     

<span id='DSSD'>  </span>

**DSSD**
{:.warning}
使用特征金字塔缓解小目标检测问题；    
```
backbone： SSD + ResNet101     
neck： 特征融合（反卷积+相乘）     
head： 使用了残差结构    
```  
结果：效率降低了 7 倍，精度就提升了 1 个点不到；  

1. 深层反卷积，浅层叠了两层标配卷积，然后再相乘的；*加了好多参数量,加法融合后 mAP 提高了 1.3，乘法再提高 0.2*     
1. 预测时的残差结构令 mAP 提升了 0.5 ～ 0.7；   

*特征融合包括：逐元素相加（FPN）、逐元素相乘（DSSD）、通道拼接*    
>
这么多的参数量，才带来 2 个百分点的提升，工业中很不值得     
对于 VOC 数据集只提升了 2 个点，但不知道实际应用中和 SSD 的差距会有多少；    

<span id='DSSD'>  </span>

**RSSD**
{:.warning}
使用特征金字塔缓解小目标检测问题；并使用彩虹连接缓解误检问题；    
```
backbone： SSD + ResNet101     
neck： 彩虹融合；     
head： 用同一个分类网络（所有框共用一套分类参数），更稳定，收敛更快；
```    

**结果**：300 的输入时，效率比 SSD 降了一半，精度也没见提高啊；512 的输入倒是提高了 1 个点；单独的小目标召回确实高了2～4个点；  

**彩虹融合**：通常下采样用来将低层特征逐层堆叠到高层去，上采样则是将高层特征逐层堆叠到低层；此处就用 pooling 和反卷积同时将上下层特征图拼接到当前层，那么所有特征图的通道数就一样了；   
特征融合前用了 BN，达到归一化感受野和不同尺度的效果`怎么就达到这个效果了，确定不是为了强行使用 BN 而杜撰的概念`{:.warning}

**issue**   
1. 最后怎么做的分类，确定不是参数多了之后引起的分类更准确？     
其实 SSD 中最后分类时，需要针对每一张特征图定义一个分类层，反向传播时自然也是单独训练；因为这些分类层不改变特征图尺寸，只改变通道数，姑且认为等于分类数量，也就意味着最终经过分类层/预测层处理后的特征图通道数是一样的；而这里，因为彩虹融合使得每个用于预测的特征图通道相同，那么在输入输出通道数一样的情况下，及可以将多个分类层可以合并成一个；
>且不说效果怎么样，至少参数量是又降低了的；    

1. 和 DSSD 相比，同样使用了 ResNet101，为啥这个速度快了很多    
难道只是因为用 concate 代替了乘法，速度就提升了 4 倍？    

<span id='RefineDet'>  </span>

**RefineDet**
{:.warning}
提升检测精度       
```
backbone： SSD + RPN     
neck： FPN（反卷积 + 相加）     
```    
SSD 搭配 RPN 先过滤掉大部分的负样本框；然后将剩下的框直接送入后续的预测——FPN + SSD；     
*借鉴了 FasterRCNN 的 RPN，但是没有 ROIPooling 层，就算不得是两阶段；*     
>虽然是在融合 FasterRCNN，但是没有在融合他最影响精度的点；     


<span id='YOLOv1'>  </span>

**YOLOv1**
{:.warning}
提升速度       
```
backbone： darknet24（神似 GoogLeNet，448×448，比 VGG 快）     
```    
**基准框**：将图片划分成 s×s 个网格，每个网格负责预测 B 个目标和 C 个类别；     
此处 B 为 2；s 为 7，最终由特征图大小控制，即特征图上一个点对应原图一个区域，而且相邻区域无重叠，这不就是滑窗策略吗；`但如果是滑窗，就应该用全卷积，而最后用了几层全连接打乱了位置信息，所以这个区域映射的思路逻辑上是不通的`{:.warning}    
**预测输出**：$B \times box + C$，其中 $box$ 为 5（x, y, w, h, confidence）；也就是说特征图上每个点预测 2 个框，这两个框有不同的位置信息，但是共用一个类别信息，最终会舍弃 confidence 低的那个物体；*此 condifence 预测的是 box 的 IoU*    
box 回归时，default box 只有一个信息就是中心坐标；回归结果中包含了中心的偏移量和宽高；那么其实可以理解为 anchor 是一组只有中心坐标、宽高为 0 的框，后续预测仍旧使用了框的概念，所以很多文献把 YOLO 归类为 anchor free 是不合理的；   
**预测框与真值框绑定**：两种框中心点落在同一格子的即绑定，和 IoU 绑定策略不同；     
FastYolo：darknet19，达到 155fps；    

**缺点**：    
1. 定位误差大`归咎于下采样率高`{:.warning}，小目标效果差、召回率低（*损失函数没着重考虑小目标，下采样率高，没有 FPN*）；   
1. 邻近目标（密集）易漏检（因为一个格子只预测两个同类别目标），比如说人群、鸟群；     
1. 非正方形目标效果略差；       

**Issue**:    
1. 怎么控制被检测物体的尺度在一个范围的，多大的范围？    
宽高回归在（0, 1），基准是图片的宽高，这样最大的目标宽高就有界限了；中心坐标也一样，以下采样尺度，即 s×s 格子的大小为限`应该是`{:.warning}     

>关于 box IoU confidence 的思路很先进啊，这个想法很多网络都没考虑，是到后来的一些检测网络才考虑到这个点可以提升 NMS 的效果；    


*为什么感觉简单的逻辑被作者说得都迷糊了*    

<span id='YOLOv2'>  </span>

**YOLOv2**
{:.warning}
提升精度       
```
backbone： darknet19      
```    
**darknet19**：    
- 输入416×416，32倍下采样；精度堪比 VGG，运算量是 1/4；    
- 3×3 代替 7×7；    
- BN 提高了 2 个百分点；   
- 去除了 fc 和 dropout；   
- 特征融合：pooling + concate，提升了 1%；`这是哪门子融合啊，别人都是把高层语义融合到下层，这个 pooling 上去有啥意义`{:.warning}     

**基准框**：使用 anchor 机制，一组 5 个；尺度也是从训练集聚类所得，选平均 IoU 最大的；    
**预测输出**：预测用 1×1 的卷积在通道维度上做，很合逻辑啊；$(class + 1 + box) \times 5$，box 为 4，丢掉了 IoU 置信度预测；    
**预测框与真实框绑定**：中心落在同一区域的就绑定；最大 IoU 的为正样本，IoU < 0.6 的为负；`奇怪`{:.warning}     
**工程技巧**：    
- 多尺度输入，没有用 fc，可以接受任意尺寸输入；*这样不用重新训练就可以随意选择精度为主还是速度为主*    
- crop + resize 实现单尺度输入下的多尺度目标；    

**缺点**：    
- 单层预测，细粒度效果一般；    
- 小物体一般；    
- 工程技巧多，导致扩展性不够好；    

**Issue**：    
1. YOLO9000 怎么预测未知类别的？    
YOLO9000 使用的是层次 softmax；所谓的未知类别，说的是物体本身的类别，而可预测的是该类别的父类别；也就是用父类别来保底；    

1. 为什么取消掉 IoU 的预测呢，不好用吗？    
1. 为什么说预测框偏移量比直接预测更容易，不就是差了个基准线吗？


<span id='YOLOv3'>  </span>

**YOLOv3**
{:.warning}
提升精度       
```
backbone： darknet53（416×416）     
neck： FPN（uppooling + concate）     
head： 多层
anchor： 聚类得到一组 9 个，每层分配三个
cls： softmax 换成了 logistic 回归（解决多标签问题）
```    
**darknet53**：     
融入了残差结构；速度直接下降；    
tiny darknet：速度和模型大小都是最佳；     
**预测输出**： $(class + 1 + box) \times 3$，`那个 1 代表背景和代表 IoU 有什么区别`{:.warning}     
**优点**：背景误检低（正样本生成严格）`什么意思`{:.warning}     
**缺点**：定位误差大，召回低，拥挤效果差；`不是都使用 anchor 了吗`{:.warning}     
业界应用多，相同精度下，推理速度更高；     

<span id='YOLOv4'>  </span>

**YOLOv4**
{:.warning}
提升精度       
技巧的堆叠，框架过于复杂了；    
**CSP**：    
- GPU：ResNeXt50，DarkNet53     
- VPU：EfficientNet-lite，MixNet，GhostNet，MobileNetV3；     

**Neck**：    
*FPN，SFAM，ASFF，BiFPN*     
- FPN：两层相加；    
- GFM：N层相加；    
- EFM：加两次，先上中下，再上中加；    

**数据增强**：颜色抖动，噪声，缩放，crip，flip，rotate，erase，mixup（全图融合），黑边扰动，cutmix（切块贴上去），cutout，mosaic（多张图拼在一起）；   
**回归**：GIoU，DIoU，CIoU；    
**增大感受野**：SPP，ASPP，RFB；    
**注意力**：SE，SpatialAM；    
**NMS**：SoftNMS，DIoUNMS；    

<span id='RetinaNet'>  </span>

**RetinaNet**
{:.warning}
样本不均衡       
```
backbone： ResNet     
neck： FPN
head：多层预测
cls_loss：focalloss
```    
FPN 增加小目标检测精度；    
focal loss 解决样本不均衡问题；    
二分类交叉熵：&emsp;&emsp;&emsp;&emsp;$$
CE(p,y)=\left\{
\begin{aligned}
&-\log (p) & if \quad y = 1 \\
&-\log (1-p) & otherwise
\end{aligned}
\right.
$$    
为了简化定义，我们定义：&emsp;&emsp;&ensp;$$
p_t=\left\{
\begin{aligned}
&p & if \quad y = 1 \\
&1-p & otherwise
\end{aligned}
\right.
$$    
则：&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$$CE(p,y) = CE(p_t) = -\log(p_t)$$    

平衡交叉熵：&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$$CE(p_t) = -\alpha_t\log(p_t)$$   
为了处理类别间不均衡的情况，我们为每一个类别赋值一个权重 $\alpha, \quad \alpha \in (0, 1), \sum_t\alpha_t = 1$；     
focal loss：&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$$FL(p_t) = -(1-p_t)^\gamma\log(p_t)$$

issue:    
1. 上采样怎么实现的，最近邻和cnn？    

### c free anchor

## B 参考资料
1. [目标检测比赛中的tricks](https://zhuanlan.zhihu.com/p/102817180)
1. [目标检测算法中检测框合并策略技术综述](https://zhuanlan.zhihu.com/p/48169867)     
1. [目标检测比赛提高mAP的方法](https://www.cnblogs.com/zi-wang/p/12537034.html)    
1. [目标检测入门，看这篇就够了](https://zhuanlan.zhihu.com/p/34142321)-glint     
