---
layout: article
title:  "「CV」 目标检测概述"
date:   2020-06-04 16:00:40 +0800
key: ObjectDetection-survey
aside:
  toc: true
category: [AI, CV, detection]
---
<span id='head'></span>
[目标检测资源](/ai/cv/detection/2019/05/10/foundation.html)     

<center class="half">
  <img src="/assets/images/cv/detection/overview.png" /><br>图1：目标检测发展路线&emsp;
</center>

<!--more-->  

# 1 技术点

# 2 经典网络
## 2.1 基于 anchor
### 2.1.1 two stage  

| 模型 | 意义 | 创新点 |
| --- | --- | --- |
| RCNN | 开创者 | 首次证明在目标检测领域深度学习方法的有效性； <br>检测精度比传统特征（Hog特征）精度更高 |
| Fast RCNN | 速度 | 使用 RoIPooling 实现区域间特征共享；<br>使用多任务损失（分类和框回归）实现了端到端的训练；|
| Faster RCNN | 速度精度 | 用 RPN 代替选择性搜索，速度提升，且与检测网络共享特征图；<br>用多尺度 anchor 来应对多尺度目标检测；无需输入多尺度特整体即可完成多尺度目标检测；|
| Mask RCNN | 精度 | 加入分割任务，同时输出检测和分割结果；<br>特征提取用 ResNet -FPN (feature pyramid network)，并在每个金字塔层独立作出预测，提升了精度；<br>RoIAlign 替换 RoIPooling；  |

### 2.1.2 one stage

| 模型 | 意义 | 创新点 |
| --- | --- | --- |
| YOLO | 实时检测 | 将输入划分为 S×S 网格，网格负责中心点落在网格内的目标；<br>框预测和分类都用回归来做； |
| YOLOv2 | 训练加速和精度 | 加了 BN<br>高分辨率输入：448×448<br>引入了 anchor，去掉了预测框用的全连接层<br>从训练集聚类得到 anchor 参数<br>多尺度特征融合<br>多尺度训练<br>Darknet 19 |
| YOLOv3 | 小目标 | 多标签分类（独立的分类器）适应重迭标签<br>在三种尺度特征图上做预测<br>特征提取用 Darknet-53 |
| SSD | 精度 | 使用多尺度特征进行预测<br>数据扩充 |
| DSSD | 精度 | backbone：ResNet-101<br>加入了 FPN |
| Retinanet | 样本不均衡 | 提出了 focal loss<br>ResNeXt-101-FPN |
| RefineDet | 小目标 | 检测分两步走 |
| M2Det | 多尺度目标 | MLFPN |

`为什么会选用不同的指标`{:.warning}     
`YOLO 和 SSD 两个系列的区别是什么`{:.warning}    


| 模型     | 创新点 | backbone | neck | head |  |
| ---     | --- | --- | --- | --- | --- |
| SSD     | 正负样本不均衡<br>速度和精度的平衡 | VGG16 | 原始特征图 | 多层预测 | |
| DSSD    | 小目标检测 | SSD+ResNet | 反卷积+高-低相乘| 残差 | |
| RSSD    | 小目标检测<br>误检| SSD+ResNet | 上下同时来 concate | 分类参数共享 | |
|  | | | | | |
| | | | | | |

## 2.2 anchor free

| 模型 | 意义 | 创新点 |
| --- | --- | --- |
| RelationNet | 物体间的关系 | 提出目标关系模块 |
| DCNv2 | 几何形变问题 | 提出可变形卷积网络 DCN |
| NAS-FPN | 精度 | 自动搜索出的 FPN 结构 |

# 3 难点
## 3.1 常规组件
### 3.1.1 骨干网络

### 3.1.2 后处理

## 3.2 精度
### 3.2.1 样本不均衡

### 3.2.2 过拟合

### 3.2.3 多尺度

### 3.2.4 遮挡&拥挤


## 3.3 性能
### 3.3.1 [轻量化模型](/ai/dl/cnn/2020/06/27/light-cnn.html)
### 3.3.2 模型压缩


# 4 优化方向

| 方向 | 策略 |
| --- | --- |
| backbone |  |
| 优化策略 | UnitBox，IoUNet |
| 损失函数 | L1,L2,focal loss |
| NMS | soft-nms，softer-nms，relation net，ConvNMS，NMSNet，YesNet |
| anchor 生成 | 滑窗，RPN，CornerNet，meta-anchor |
| 零样本 |  |
|  |  |
|  |  |
|  |  |


# 5 发展
小目标的检测(如小于30像素的目标物体)、遮挡面积较大的目标以及区分图像中与目标物体外形相似的非目标物体等问题;   
实时性:对于自动驾驶或汽车辅助驾驶等场景对实时处理能力要求较高；   
小数据量：微调的精度尚待提高；    
特定行业很难获取大量的监督数据；    

-------------------  
[End](#head)
{:.warning}  


# 附录
## A 经典网络
### a 两阶段

### b 单阶段
<span id='SSD'>  </span>

**SSD**
{:.warning}
```
backbone： VGG16(conv4-3 之后的去掉了)   
neck： 单层特征图     
head： 多层预测；添加了c8（10×10）、c9（5×5）、c10（3×3）、c11（1×1） 联合 conv4-3（19×19） 一起做预测；    
添加的几层是先用 1×1 降通道，然后再降分辨率；     
损失函数： softmax-交叉熵，SmoothL1   
prior box：    
```
**triker：**    
1. 解决正负样本不均衡    
筛选出 1：3，`怎么做的`{:.warning}     
1. 丰富的数据增强`都有什么`{:.warning}；     
1. 为了增加感受野，且不增加参数、保持特征数不变，而使用了 3×3，step 1 的池化`确定 step 是 1`{:.warning}和空洞卷及（空洞 6，，padding 6）；`为什么要增加那么多感受野呢`{:.warning}    
1. 所有类别共享一个回归值，FasterRCNN 是不同类别用不同回归值`什么意思`{:.warning}     

**缺点：**    
1. 小物体检测不好，低层特征虽然有利于定位，但是语义信息不够，所以分类不够好；   
1. 先验框尺寸需要手动设定，要根据不同任务去修改`怎么改`{:.warning}     
1. 一阶段算法的精度还是比两阶段的低；   

**issue：**     
1. default box 与 真是框的匹配策略     
双层 for 循环，取 IoU 最大且大于 0.5 的为正；`到底啥情况`{:.warning}    
进行前向传播，然后根据预测框和真实框的 IoU 将预测框分为正负（>0.5, 策略同 FasterRCNN），筛选出正样本（一般不超过 100 个），正负样本比例 1：3；     
然后是将预测框与真实框进行绑定：预测框绑定到与其 IoU 最大的真实框上，真实框要与其 IoU 最大的预测框绑定（为了提高召回率）；`两个规则有冲突怎么办，是否有顺序关系`{:.warning}      
反向传播时只取正样本的偏移量，负样本的舍弃；   
1. 为什么说借鉴了 FasterRCNN 的 RPN 思路；    
RPN 中特征图山一个点对应原图中的一组框，只不过这组框在 SSD 中更名为 prior box；                          
RPN 是分两段：先对 box 粗回归 + 二分类（前背景），筛选过后进行二次回归 + 多分类；SSD 是只有第二步，直接进行多分类 + box 回归；  
1. 框是怎么映射回去的    
1. 怎么定义正负样本，并控制比例在 1：3 的    
1. 为什么不用 IoU 做损失    
可以用，而且抗尺度影响；    

*派生算法有：DSSD（上采样特征融合）、RSSD（彩虹特征融合）、RefineDet（FasterRCNN 结合 SSD）、RFBNet（多个感受野融合）*     

<span id='DSSD'>  </span>

**DSSD**
{:.warning}
使用特征金字塔缓解小目标检测问题；    
```
backbone： SSD + ResNet101     
neck： 特征融合（反卷积+相乘）     
head： 使用了残差结构    
```  
效率降低了 7 倍，精度就提升了 1 个点不到；  
1. 深层反卷积，浅层叠了两层标配卷积，然后再相乘的；*加了好多参数量,加法融合后 mAP 提高了 1.3，乘法再提高 0.2*     
1. 预测时的残差结构令 mAP 提升了 0.5 ～ 0.7；   

*特征融合包括：逐元素相加（FPN）、逐元素相乘（DSSD）、通道拼接*    
>
这么多的参数量，才带来 2 个百分点的提升，工业中很不值得     
对于 VOC 数据集只提升了 2 个点，不知道实际应用中和 SSD 的差距又有多少；    

<span id='DSSD'>  </span>

**RSSD**
{:.warning}
使用特征金字塔缓解小目标检测问题；并使用彩虹连接缓解误检问题；    
```
backbone： SSD + ResNet101     
neck： 彩虹融合；     
head： 用同一个分类网络（所有框共用一套分类参数），更稳定，收敛更快；
```    
`最后怎么做的分类`{:.warning}    
300 的输入时，效率比 SSD 降了一半，精度也没见提高啊；512 的输入倒是提高了 1 个点；单独的小目标召回确实高了2～4个点；  

彩虹融合：通常下采样用来将低层特征逐层堆叠到高层去，上采样则是将高层特征逐层堆叠到低层；此处就用 pooling 和反卷积同时将上下层特征图拼接到当前层，那么所有特征图的通道数就一样了；   
特征融合前用了 BN，达到归一化感受野和不同尺度的效果`怎么就达到这个效果了，确定不是为了强行使用 BN`{:.warning}

**issue**   
1. 和 DSSD 相比，同样使用了 ResNet101，为啥这个速度快了很多    
难道只是因为用 concate 代替了乘法，速度就提升了 4 倍？    

### c free anchor

## B 参考资料
1. [目标检测比赛中的tricks](https://zhuanlan.zhihu.com/p/102817180)
1. [目标检测算法中检测框合并策略技术综述](https://zhuanlan.zhihu.com/p/48169867)     
1. [目标检测比赛提高mAP的方法](https://www.cnblogs.com/zi-wang/p/12537034.html)    
1. [目标检测入门，看这篇就够了](https://zhuanlan.zhihu.com/p/34142321)-glint     
